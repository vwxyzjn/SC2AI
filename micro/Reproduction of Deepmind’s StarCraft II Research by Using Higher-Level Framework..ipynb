{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproduction of Deepmindâ€™s StarCraft II Research by Using Higher-Level Framework\n",
    "\n",
    "Author: Shengyi (Costa) Huang, Costa.Huang@outlook.com\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In early 2017, Google Deepmind introduced a python library [``Pysc2``](https://github.com/deepmind/pysc2), the SC2LE (StarCraft II Learning Environment). It provides a interface for RL (Reinforcement Learning) agents to interact with StarCraft 2 by providing the observations and receiving actions. In their paper, Deepmind also described some baseline training algorithms and used them to train agents in [mini-games](https://github.com/deepmind/pysc2/blob/master/docs/mini_games.md) such as ``BuildMarines``, ``DefeatRoaches`` and ``MoveToBeacon``<cite data-cite=\"Vinyals2017-ck\"> (Vinyals, 2017)</cite>. Nevertheless, the implementation of the algorithm is not revealed. In this research project, we surveys some reproduction of Deepmind's result and found most of their implementation hard to understand and reproduce. In light of this, I reproduces the results by using higher-level framework (e.g. Tensorforce and gym) to enhance maintainability and understandibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Mechanics of Pysc2\n",
    "\n",
    "One of the most helpful tutorial is from [Building a Basic PySC2 Agent](https://chatbotslife.com/building-a-basic-pysc2-agent-b109cde1477c) and [Building a Smart PySC2 Agent](https://chatbotslife.com/building-a-smart-pysc2-agent-cdc269cb095d). In the simplest form, you create a class that inherits from ``base_agent.BaseAgent`` and override the ``step()`` function. In essence, the ``step`` function of ``base_agent.BaseAgent`` gives you the observation of current state, including units killed, rewards, and etc, and you need to return an action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pysc2.agents import base_agent\n",
    "from pysc2.lib import actions\n",
    "\n",
    "class SimpleAgent(base_agent.BaseAgent):\n",
    "    def step(self, obs):\n",
    "        super(SimpleAgent, self).step(obs)\n",
    "        \n",
    "        return actions.FunctionCall(actions.FUNCTIONS.no_op.id, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you follow the author's instruction and create a [``simple_agent.py``](https://github.com/skjb/pysc2-tutorial/blob/master/Building%20a%20Basic%20Agent/simple_agent.py) and run\n",
    "```cmd\n",
    "python -m pysc2.bin.agent \\\n",
    "--map Simple64 \\\n",
    "--agent simple_agent.SimpleAgent \\\n",
    "--agent_race T\n",
    "```\n",
    "\n",
    "You will get something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/vd-LxvKmnX8\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/vd-LxvKmnX8\" frameborder=\"0\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning\n",
    "\n",
    "Definition: reinforcement learning trains the agent to maximize accumulated rewards.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"screenshots/Reinforcement Learning.PNG\" height=\"600\" width=\"600\" />\n",
    "<br>\n",
    "<span><i>Powerpoint. The visualization of RL by David Silver</i></span>\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"screenshots/Reinforcement Learning example.PNG\" height=\"600\" width=\"600\" />\n",
    "<br>\n",
    "<span><i>Powerpoint. A RL example by David Silver</i></span>\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"screenshots/Reinforcement optimal policy.PNG\" height=\"600\" width=\"600\" />\n",
    "<br>\n",
    "<span><i>Powerpoint. The policy of an agent by David Silver</i></span>\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"screenshots/Reinforcement Value Function.PNG\" height=\"600\" width=\"600\" />\n",
    "<br>\n",
    "<span><i>Powerpoint. The value function of an agent by David Silver</i></span>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor critic algorithm\n",
    "\n",
    "Actor is the \"policy\" of the agent, while Critic is the \"value function\" of the agent. The actor-only algorithm usually suffers from slow learning, while the critic-only algorithm had issues dealing with continuous action spaces<cite data-cite=\"Grondman2012-ve\"> (Grondman, 2012)</cite>. Futhermore, according to Grondman, the actor-critic algorithm combines the best of the both world and possesses the following advantages:\n",
    "\n",
    "* Faster training time\n",
    "* Better handling of continuous actions\n",
    "* (Usually) Nice convergence properties\n",
    "\n",
    "### A3C\n",
    "\n",
    "A3C specifies a procedure that asynchronously trains multiple agents in parallel, on multiple\n",
    "instances of the environment. Such procedure surpasses the previous state-of-the-art on the Atari domain while \"training for half the time on a single multi-core CPU instead of a GPU\" <cite data-cite=\"Mnih2016-uo\"> (Mnih, 2016)</cite>. In an intuitive way, one can imagine multiple agents being trained in parallel universe and reflect their experience to the model asynchronously, hence resulting in a more generalized training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Existing Reproduction\n",
    "\n",
    "Some of the popular (most starred) reproduction repos are but not limited to:\n",
    "* [pysc2-examples](https://github.com/chris-chris/pysc2-examples)\n",
    "* [pysc2-agents](https://github.com/xhujoy/pysc2-agents)\n",
    "* [sc2aibot](https://github.com/pekaalto/sc2aibot)\n",
    "* [pysc2-RLagents](https://github.com/greentfrapp/pysc2-RLagents)\n",
    "\n",
    "Unfortunately, most of the implementation are very hard to understand. Common issues includes \n",
    "\n",
    "* Unconventional project setup\n",
    "    * For example, ``pysc2-examples``, one of the most starred reproduction, does not follow the typical python project structure defined [here](https://packaging.python.org/tutorials/distributing-packages/#initial-files). As a result, it seems that one can only use Pycharm to run the project.\n",
    "* Poor documentation\n",
    "    * For example, ``sc2aibot`` only provides a documentation for the [agent parameters](https://github.com/pekaalto/sc2aibot/blob/master/actorcritic/agent.py), and all the other functions are left undocumented. Because of this, it's even hard to figure out how the observation is passed to the model.\n",
    "* Tight coupling/ Large functions\n",
    "    * For example, ``pysc2-RLagents`` use [one giant file](https://github.com/greentfrapp/pysc2-RLagents/blob/master/Agents/PySC2_A3C_AtariNet.py) to include \"everything\" (the training, runing, reacting components of the algorithm). Deeply nested loop and condition are ubiquitous and, as a result, the author's work is almost unreadable. \n",
    "    \n",
    "Nonetheless, I would like to point out that ``sc2aibot`` is probably the best out of those repos. Unlike other repos,  ``sc2aibot`` almost reproduces most of the Mini-games and shows a decent results compared to Deepmind's results:\n",
    "\n",
    "\n",
    "<center>\n",
    "<br>\n",
    "<table align=\"center\">\n",
    "  <tr>\n",
    "        <td align=\"center\">Map</td>\n",
    "        <td align=\"center\">Avg score</td>\n",
    "        <td align=\"center\">Deepmind avg</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td align=\"center\">MoveToBeacon</td>\n",
    "        <td align=\"center\">25</td>\n",
    "        <td align=\"center\">26</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td align=\"center\">CollectMineralShards</td>\n",
    "        <td align=\"center\">91</td>\n",
    "        <td align=\"center\">103</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td align=\"center\">DefeatZerglingsAndBanelings</td>\n",
    "      <td align=\"center\">48</td>\n",
    "      <td align=\"center\">62</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td align=\"center\">FindAndDefeatZerglings</td>\n",
    "      <td align=\"center\">42</td>\n",
    "      <td align=\"center\">45</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td align=\"center\">DefeatRoaches</td>\n",
    "      <td align=\"center\">70-90</td>\n",
    "      <td align=\"center\">100</td>\n",
    "    </tr>\n",
    "</table>\n",
    "<br>\n",
    "<span><i>Table. The comparision between the results from Deepmind and sc2aibot</i></span>\n",
    "</center>\n",
    "\n",
    "I ran ``sc2aibot`` implementation on ``CollectMineralShards`` mini-games for 12 hours, yet the environment only ran for 5,000 episodes and the average reward is still around 20. Since he had already ran such algorithms with 56,000 episodes, I was not interested in running it.\n",
    "\n",
    "<center>\n",
    "<img src=\"screenshots/sc2aibot_CollectMineralShards_training_time.png\" width=\"600\" height=\"600\" />\n",
    "<br>\n",
    "<span><i>Picture. The training episodes (x-axis) and scores (y-axis)</i></span>\n",
    "</center>\n",
    "\n",
    "Notice in his screenshot, the training time only took about 8 hours. This is probably due to his more powerful computer and parallel trainings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproduction through OpenAI Gym and Tensorforce\n",
    "\n",
    "From a software engineering perspective, we want to write modular code for maintainabiliy and understandibily. Because of this, we want to seperate the task of training agents into two subsystem: the system that deals with gaming environment and the system that deals with agents training.\n",
    "\n",
    "### The subsystem that deals with the gaming environment\n",
    "\n",
    "Fortunately, there exist such systems/librarys that are well structured and documented. For example, OpenAI Gym is a python RL toolkit that gives you access to a standardized set of environments <cite data-cite=\"Brockman2016-dq\"> (Brockman, 2016)</cite>. Each environment is expected to have a standardized set of methods for user to call. A typical environment would look like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# https://github.com/openai/gym/tree/master/gym/envs\n",
    "import gym\n",
    "from gym import error, spaces, utils\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "\n",
    "class FooEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        self.observation_space = spaces.Box(low, high)\n",
    "\n",
    "    def _step(self, action):\n",
    "        pass\n",
    "    \n",
    "    def _reset(self):\n",
    "        pass\n",
    "    \n",
    "    def _render(self, mode='human', close=False):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of such setup, one can find out explicitly what the valid actions and observation look like by using ``action_space`` and ``observation_space``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_space = spaces.Discrete(2)\n",
    "# The action_space only has two discrete actions: 0 and 1\n",
    "print(action_space)\n",
    "action_space.contains(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using OpenAI Gym, we enhance the robostness and predictability of the environment. I was planning on creating a gym wrapper/binding with ``pysc2``, but luckily someone has already made such library: [``sc2gym``](https://github.com/islamelnabarawy/sc2gym). It enables us \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#blablabla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The subsytem that deals with the training process\n",
    "\n",
    "One of the shortfall of the existing reproduction is that they hard-coded the training process includes predicting an action, analyzing rewards, updating action policy and etc. Instead of reinventing the wheels, we should use reinforcement learning libraries such as Tensorforce and Keras-Rl that are readily available. Their training process is much more robost because the underlying codebase is usually well-tested.\n",
    "\n",
    "After much comparison, I chose Tensorforce because of its flexibility.\n",
    "> TensorForce is an open source reinforcement learning library focused on providing clear APIs, readability and modularisation to deploy reinforcement learning solutions both in research and practice. <cite data-cite=\"Schaarschmidt2017-ur\"> (Schaarschmidt, 2017)</cite>\n",
    "\n",
    "A typical example involves Openai Gym looks like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Blas GEMM launch failed : a.shape=(1, 4), b.shape=(4, 32), m=1, n=32, k=4\n\t [[Node: actions-and-internals/apply/apply/apply/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](_arg_state_0_1/_113, ppo/actions-and-internals/layered-network/apply/dense0/apply/linear/apply/W/read)]]\n\t [[Node: actions-and-internals/sample/Select/_129 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_75_actions-and-internals/sample/Select\", tensor_type=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'actions-and-internals/apply/apply/apply/MatMul', defined at:\n  File \"C:\\Users\\costa\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\costa\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\costa\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\costa\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\costa\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\costa\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\costa\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\costa\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\costa\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\costa\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\costa\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\costa\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\costa\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\costa\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\costa\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\costa\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\costa\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\costa\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\costa\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\costa\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-22-e01ddb7d3331>\", line 49, in <module>\n    distributed_spec=None\n  File \"f:\\grad school\\tensorforce\\tensorforce\\tensorforce\\agents\\ppo_agent.py\", line 155, in __init__\n    keep_last_timestep=keep_last_timestep\n  File \"f:\\grad school\\tensorforce\\tensorforce\\tensorforce\\agents\\batch_agent.py\", line 72, in __init__\n    batched_observe=batched_observe\n  File \"f:\\grad school\\tensorforce\\tensorforce\\tensorforce\\agents\\agent.py\", line 134, in __init__\n    actions_spec=self.actions_spec,\n  File \"f:\\grad school\\tensorforce\\tensorforce\\tensorforce\\agents\\ppo_agent.py\", line 178, in initialize_model\n    likelihood_ratio_clipping=self.likelihood_ratio_clipping\n  File \"f:\\grad school\\tensorforce\\tensorforce\\tensorforce\\models\\pg_prob_ratio_model.py\", line 75, in __init__\n    gae_lambda=gae_lambda,\n  File \"f:\\grad school\\tensorforce\\tensorforce\\tensorforce\\models\\pg_model.py\", line 80, in __init__\n    entropy_regularization=entropy_regularization,\n  File \"f:\\grad school\\tensorforce\\tensorforce\\tensorforce\\models\\distribution_model.py\", line 69, in __init__\n    variable_noise=variable_noise\n  File \"f:\\grad school\\tensorforce\\tensorforce\\tensorforce\\models\\model.py\", line 113, in __init__\n    self.setup()\n  File \"f:\\grad school\\tensorforce\\tensorforce\\tensorforce\\models\\model.py\", line 228, in setup\n    deterministic=self.deterministic_input\n  File \"f:\\grad school\\tensorforce\\tensorforce\\tensorforce\\models\\model.py\", line 765, in create_output_operations\n    deterministic=deterministic\n  File \"C:\\Users\\costa\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\template.py\", line 261, in __call__\n    return self._call_func(args, kwargs, check_for_new_variables=True)\n  File \"C:\\Users\\costa\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\template.py\", line 217, in _call_func\n    result = self._func(*args, **kwargs)\n  File \"f:\\grad school\\tensorforce\\tensorforce\\tensorforce\\models\\distribution_model.py\", line 144, in tf_actions_and_internals\n    embedding, internals = self.network.apply(x=states, internals=internals, update=update, return_internals=True)\n  File \"C:\\Users\\costa\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\template.py\", line 261, in __call__\n    return self._call_func(args, kwargs, check_for_new_variables=True)\n  File \"C:\\Users\\costa\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\template.py\", line 217, in _call_func\n    result = self._func(*args, **kwargs)\n  File \"f:\\grad school\\tensorforce\\tensorforce\\tensorforce\\core\\networks\\network.py\", line 241, in tf_apply\n    x = layer.apply(x, update, *layer_internals)\n  File \"C:\\Users\\costa\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\template.py\", line 261, in __call__\n    return self._call_func(args, kwargs, check_for_new_variables=True)\n  File \"C:\\Users\\costa\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\template.py\", line 217, in _call_func\n    result = self._func(*args, **kwargs)\n  File \"f:\\grad school\\tensorforce\\tensorforce\\tensorforce\\core\\networks\\layer.py\", line 513, in tf_apply\n    xl1 = self.linear.apply(x=x, update=update)\n  File \"C:\\Users\\costa\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\template.py\", line 261, in __call__\n    return self._call_func(args, kwargs, check_for_new_variables=True)\n  File \"C:\\Users\\costa\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\template.py\", line 217, in _call_func\n    result = self._func(*args, **kwargs)\n  File \"f:\\grad school\\tensorforce\\tensorforce\\tensorforce\\core\\networks\\layer.py\", line 436, in tf_apply\n    x = tf.matmul(a=x, b=self.weights)\n  File \"C:\\Users\\costa\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 1891, in matmul\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n  File \"C:\\Users\\costa\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 2436, in _mat_mul\n    name=name)\n  File \"C:\\Users\\costa\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\costa\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\costa\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInternalError (see above for traceback): Blas GEMM launch failed : a.shape=(1, 4), b.shape=(4, 32), m=1, n=32, k=4\n\t [[Node: actions-and-internals/apply/apply/apply/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](_arg_state_0_1/_113, ppo/actions-and-internals/layered-network/apply/dense0/apply/linear/apply/W/read)]]\n\t [[Node: actions-and-internals/sample/Select/_129 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_75_actions-and-internals/sample/Select\", tensor_type=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32mC:\\Users\\costa\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\costa\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\costa\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: Blas GEMM launch failed : a.shape=(1, 4), b.shape=(4, 32), m=1, n=32, k=4\n\t [[Node: actions-and-internals/apply/apply/apply/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](_arg_state_0_1/_113, ppo/actions-and-internals/layered-network/apply/dense0/apply/linear/apply/W/read)]]\n\t [[Node: actions-and-internals/sample/Select/_129 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_75_actions-and-internals/sample/Select\", tensor_type=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-e01ddb7d3331>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[1;31m# Start learning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m \u001b[0mrunner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_episode_timesteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepisode_finished\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepisode_finished\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[1;31m# Print statistics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mf:\\grad school\\tensorforce\\tensorforce\\tensorforce\\execution\\runner.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, timesteps, episodes, max_episode_timesteps, deterministic, episode_finished)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m                 \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat_actions\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mf:\\grad school\\tensorforce\\tensorforce\\tensorforce\\agents\\agent.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, states, deterministic)\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0mstates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0minternals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_internals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             \u001b[0mdeterministic\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         )\n\u001b[1;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mf:\\grad school\\tensorforce\\tensorforce\\tensorforce\\models\\model.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, states, internals, deterministic)\u001b[0m\n\u001b[1;32m    863\u001b[0m         \u001b[0mfeed_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_input\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 865\u001b[0;31m         \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minternals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimestep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmonitored_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    866\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mbatched\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\costa\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    519\u001b[0m                           \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                           \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m                           run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m    522\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mshould_stop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\costa\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    965\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0moriginal_exc_info\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 967\u001b[0;31m         \u001b[1;32mraise\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0moriginal_exc_info\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    968\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\costa\\Anaconda3\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    691\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m             \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\costa\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    953\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m       \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\costa\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1022\u001b[0m                                   \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m                                   \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m                                   run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\costa\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    825\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 827\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    828\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\costa\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\costa\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\costa\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\costa\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: Blas GEMM launch failed : a.shape=(1, 4), b.shape=(4, 32), m=1, n=32, k=4\n\t [[Node: actions-and-internals/apply/apply/apply/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](_arg_state_0_1/_113, ppo/actions-and-internals/layered-network/apply/dense0/apply/linear/apply/W/read)]]\n\t [[Node: actions-and-internals/sample/Select/_129 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_75_actions-and-internals/sample/Select\", tensor_type=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'actions-and-internals/apply/apply/apply/MatMul', defined at:\n  File \"C:\\Users\\costa\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\costa\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\costa\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\costa\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\costa\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\costa\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\costa\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\costa\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\costa\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\costa\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\costa\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\costa\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\costa\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\costa\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\costa\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\costa\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\costa\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\costa\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\costa\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\costa\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-22-e01ddb7d3331>\", line 49, in <module>\n    distributed_spec=None\n  File \"f:\\grad school\\tensorforce\\tensorforce\\tensorforce\\agents\\ppo_agent.py\", line 155, in __init__\n    keep_last_timestep=keep_last_timestep\n  File \"f:\\grad school\\tensorforce\\tensorforce\\tensorforce\\agents\\batch_agent.py\", line 72, in __init__\n    batched_observe=batched_observe\n  File \"f:\\grad school\\tensorforce\\tensorforce\\tensorforce\\agents\\agent.py\", line 134, in __init__\n    actions_spec=self.actions_spec,\n  File \"f:\\grad school\\tensorforce\\tensorforce\\tensorforce\\agents\\ppo_agent.py\", line 178, in initialize_model\n    likelihood_ratio_clipping=self.likelihood_ratio_clipping\n  File \"f:\\grad school\\tensorforce\\tensorforce\\tensorforce\\models\\pg_prob_ratio_model.py\", line 75, in __init__\n    gae_lambda=gae_lambda,\n  File \"f:\\grad school\\tensorforce\\tensorforce\\tensorforce\\models\\pg_model.py\", line 80, in __init__\n    entropy_regularization=entropy_regularization,\n  File \"f:\\grad school\\tensorforce\\tensorforce\\tensorforce\\models\\distribution_model.py\", line 69, in __init__\n    variable_noise=variable_noise\n  File \"f:\\grad school\\tensorforce\\tensorforce\\tensorforce\\models\\model.py\", line 113, in __init__\n    self.setup()\n  File \"f:\\grad school\\tensorforce\\tensorforce\\tensorforce\\models\\model.py\", line 228, in setup\n    deterministic=self.deterministic_input\n  File \"f:\\grad school\\tensorforce\\tensorforce\\tensorforce\\models\\model.py\", line 765, in create_output_operations\n    deterministic=deterministic\n  File \"C:\\Users\\costa\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\template.py\", line 261, in __call__\n    return self._call_func(args, kwargs, check_for_new_variables=True)\n  File \"C:\\Users\\costa\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\template.py\", line 217, in _call_func\n    result = self._func(*args, **kwargs)\n  File \"f:\\grad school\\tensorforce\\tensorforce\\tensorforce\\models\\distribution_model.py\", line 144, in tf_actions_and_internals\n    embedding, internals = self.network.apply(x=states, internals=internals, update=update, return_internals=True)\n  File \"C:\\Users\\costa\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\template.py\", line 261, in __call__\n    return self._call_func(args, kwargs, check_for_new_variables=True)\n  File \"C:\\Users\\costa\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\template.py\", line 217, in _call_func\n    result = self._func(*args, **kwargs)\n  File \"f:\\grad school\\tensorforce\\tensorforce\\tensorforce\\core\\networks\\network.py\", line 241, in tf_apply\n    x = layer.apply(x, update, *layer_internals)\n  File \"C:\\Users\\costa\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\template.py\", line 261, in __call__\n    return self._call_func(args, kwargs, check_for_new_variables=True)\n  File \"C:\\Users\\costa\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\template.py\", line 217, in _call_func\n    result = self._func(*args, **kwargs)\n  File \"f:\\grad school\\tensorforce\\tensorforce\\tensorforce\\core\\networks\\layer.py\", line 513, in tf_apply\n    xl1 = self.linear.apply(x=x, update=update)\n  File \"C:\\Users\\costa\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\template.py\", line 261, in __call__\n    return self._call_func(args, kwargs, check_for_new_variables=True)\n  File \"C:\\Users\\costa\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\template.py\", line 217, in _call_func\n    result = self._func(*args, **kwargs)\n  File \"f:\\grad school\\tensorforce\\tensorforce\\tensorforce\\core\\networks\\layer.py\", line 436, in tf_apply\n    x = tf.matmul(a=x, b=self.weights)\n  File \"C:\\Users\\costa\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 1891, in matmul\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n  File \"C:\\Users\\costa\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 2436, in _mat_mul\n    name=name)\n  File \"C:\\Users\\costa\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\costa\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\costa\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInternalError (see above for traceback): Blas GEMM launch failed : a.shape=(1, 4), b.shape=(4, 32), m=1, n=32, k=4\n\t [[Node: actions-and-internals/apply/apply/apply/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](_arg_state_0_1/_113, ppo/actions-and-internals/layered-network/apply/dense0/apply/linear/apply/W/read)]]\n\t [[Node: actions-and-internals/sample/Select/_129 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_75_actions-and-internals/sample/Select\", tensor_type=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/reinforceio/tensorforce/blob/master/examples/quickstart.py\n",
    "import numpy as np\n",
    "\n",
    "from tensorforce.agents import PPOAgent\n",
    "from tensorforce.execution import Runner\n",
    "from tensorforce.contrib.openai_gym import OpenAIGym\n",
    "\n",
    "# Create an OpenAIgym environment\n",
    "env = OpenAIGym('CartPole-v0', visualize=True)\n",
    "\n",
    "# Network as list of layers\n",
    "network_spec = [\n",
    "    dict(type='dense', size=32, activation='tanh'),\n",
    "    dict(type='dense', size=32, activation='tanh')\n",
    "]\n",
    "\n",
    "agent = PPOAgent(\n",
    "    states_spec=env.states,\n",
    "    actions_spec=env.actions,\n",
    "    network_spec=network_spec,\n",
    "    batch_size=4096,\n",
    "    # Agent\n",
    "    preprocessing=None,\n",
    "    exploration=None,\n",
    "    reward_preprocessing=None,\n",
    "    # BatchAgent\n",
    "    keep_last_timestep=True,\n",
    "    # PPOAgent\n",
    "    step_optimizer=dict(\n",
    "        type='adam',\n",
    "        learning_rate=1e-3\n",
    "    ),\n",
    "    optimization_steps=10,\n",
    "    # Model\n",
    "    scope='ppo',\n",
    "    discount=0.99,\n",
    "    # DistributionModel\n",
    "    distributions_spec=None,\n",
    "    entropy_regularization=0.01,\n",
    "    # PGModel\n",
    "    baseline_mode=None,\n",
    "    baseline=None,\n",
    "    baseline_optimizer=None,\n",
    "    gae_lambda=None,\n",
    "    normalize_rewards=False,\n",
    "    # PGLRModel\n",
    "    likelihood_ratio_clipping=0.2,\n",
    "    summary_spec=None,\n",
    "    distributed_spec=None\n",
    ")\n",
    "\n",
    "# Create the runner\n",
    "runner = Runner(agent=agent, environment=env)\n",
    "\n",
    "\n",
    "# Callback function printing episode statistics\n",
    "def episode_finished(r):\n",
    "    print(\"Finished episode {ep} after {ts} timesteps (reward: {reward})\".format(ep=r.episode, ts=r.episode_timestep,\n",
    "                                                                                 reward=r.episode_rewards[-1]))\n",
    "    return True\n",
    "\n",
    "\n",
    "# Start learning\n",
    "runner.run(episodes=10, max_episode_timesteps=200, episode_finished=episode_finished)\n",
    "\n",
    "# Print statistics\n",
    "print(\"Learning finished. Total episodes: {ep}. Average reward of last 100 episodes: {ar}.\".format(\n",
    "    ep=runner.episode,\n",
    "    ar=np.mean(runner.episode_rewards[-100:]))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the way, the reason you can use ``env = OpenAIGym('CartPole-v0', visualize=True)`` to visualzie the training process of OpenAI Gym with tensorforce is because I made a [pull request](https://github.com/reinforceio/tensorforce/pull/242). You are welcome :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Attempt\n",
    "\n",
    "With the tools ready, I gave it a try with the following code. (Don't try to run the code in jupyter notebook, because it just keeps printing stuff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tensorforce.agents import PPOAgent\n",
    "from tensorforce.execution import Runner\n",
    "from tensorforce.contrib.openai_gym import OpenAIGym\n",
    "\n",
    "import sc2gym\n",
    "from absl import flags\n",
    "FLAGS = flags.FLAGS\n",
    "FLAGS([__file__])\n",
    "\n",
    "\n",
    "# Create an OpenAIgym environment\n",
    "# ReversedAddition-v0\n",
    "# CartPole-v0\n",
    "env = OpenAIGym('SC2CollectMineralShards-v2', visualize=False)\n",
    "\n",
    "# Network as list of layers\n",
    "network_spec = [\n",
    "    dict(type='conv2d', size=32),\n",
    "    dict(type='flatten'),\n",
    "    dict(type='dense', size=32, activation='relu'),\n",
    "    dict(type='lstm', size=32)\n",
    "]\n",
    "\n",
    "saver_spec = {\n",
    "    'load': True,\n",
    "    'file': 'model.ckpt-7914479',\n",
    "    'directory': './model',\n",
    "    'seconds': 3600\n",
    "}\n",
    "\n",
    "agent = PPOAgent(\n",
    "    states_spec=env.states,\n",
    "    actions_spec=env.actions,\n",
    "    network_spec=network_spec,\n",
    "    batch_size=10,\n",
    "    # Agent\n",
    "    preprocessing=None,\n",
    "    exploration=None,\n",
    "    reward_preprocessing=None,\n",
    "    saver_spec=saver_spec,\n",
    "    # BatchAgent\n",
    "    keep_last_timestep=True,\n",
    "    # PPOAgent\n",
    "    step_optimizer=dict(\n",
    "        type='adam',\n",
    "        learning_rate=1e-4,\n",
    "        epsilon=5e-7\n",
    "    ),\n",
    "    optimization_steps=10,\n",
    "    # Model\n",
    "    scope='ppo',\n",
    "    discount=0.99,\n",
    "    # DistributionModel\n",
    "    distributions_spec=None,\n",
    "    entropy_regularization=0.01,\n",
    "    # PGModel\n",
    "    baseline_mode=None,\n",
    "    baseline=None,\n",
    "    baseline_optimizer=None,\n",
    "    gae_lambda=None,\n",
    "    normalize_rewards=False,\n",
    "    # PGLRModel\n",
    "    likelihood_ratio_clipping=0.2,\n",
    "    summary_spec=None,\n",
    "    distributed_spec=None\n",
    ")\n",
    "    \n",
    "print('partially success')\n",
    "\n",
    "# Create the runner\n",
    "runner = Runner(agent=agent, environment=env)\n",
    "\n",
    "\n",
    "# Callback function printing episode statistics\n",
    "rewards = []\n",
    "def episode_finished(r):\n",
    "    print(\"Finished episode {ep} after {ts} timesteps (reward: {reward})\".format(ep=r.episode, ts=r.episode_timestep,\n",
    "                                                                                 reward=r.episode_rewards[-1]))\n",
    "    global rewards\n",
    "    rewards += [r.episode_rewards[-1]]\n",
    "    return True\n",
    "\n",
    "\n",
    "# Start learning\n",
    "runner.run(episodes=60000, episode_finished=episode_finished)\n",
    "\n",
    "# Print statistics\n",
    "print(\"Learning finished. Total episodes: {ep}. Average reward of last 100 episodes: {ar}.\".format(\n",
    "    ep=runner.episode,\n",
    "    ar=np.mean(runner.episode_rewards[-100:]))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Attempt\n",
    "\n",
    "After running the first algorithm for roughly 5 days unstoppingly, the average score is still about 20, and I lost faith on model's ability to ever jump out of the local max. As a result, I stopped the program and ran it with a different set of hyperparameters.\n",
    "\n",
    "<center>\n",
    "<img src=\"screenshots/Hyperparameter_Tuning_web.png\" />\n",
    "<br>\n",
    "<span><i>Picture. The definition of Hyperparameter Tuning by Chris Albon</i></span>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproduction through "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style> \n",
       "code {\n",
       "    background-color : #eff0f1 !important;\n",
       "    padding: 1px 5px !important;\n",
       "}\n",
       "\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<style> \n",
    "code {\n",
    "    background-color : #eff0f1 !important;\n",
    "    padding: 1px 5px !important;\n",
    "}\n",
    "\n",
    "</style>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
