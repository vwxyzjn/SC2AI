{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproduction of Deepmindâ€™s StarCraft II Research by Using Higher-Level Framework\n",
    "\n",
    "Author: Shengyi (Costa) Huang, Costa.Huang@outlook.com\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In early 2017, Google Deepmind introduced a python library [``Pysc2``](https://github.com/deepmind/pysc2), the SC2LE (StarCraft II Learning Environment). It provides a interface for RL (Reinforcement Learning) agents to interact with StarCraft 2 by providing the observations and receiving actions. In their paper, Deepmind also described some baseline training algorithms and used them to train agents in [mini-games](https://github.com/deepmind/pysc2/blob/master/docs/mini_games.md) such as ``BuildMarines``, ``DefeatRoaches`` and ``MoveToBeacon``<cite data-cite=\"Vinyals2017-ck\"> (Vinyals, 2017)</cite>. Nevertheless, the implementation of the algorithm is not revealed. In this research project, we surveys some reproduction of Deepmind's result and found most of their implementation hard to understand and reproduce. In light of this, I reproduces the results by using higher-level framework (e.g. Tensorforce and gym) to enhance maintainability and understandibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Mechanics of Pysc2\n",
    "\n",
    "One of the most helpful tutorial is from [Building a Basic PySC2 Agent](https://chatbotslife.com/building-a-basic-pysc2-agent-b109cde1477c) and [Building a Smart PySC2 Agent](https://chatbotslife.com/building-a-smart-pysc2-agent-cdc269cb095d). In the simplest form, you create a class that inherits from ``base_agent.BaseAgent`` and override the ``step()`` function. In essence, the ``step`` function of ``base_agent.BaseAgent`` gives you the observation of current state, including units killed, rewards, and etc, and you need to return an action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pysc2.agents import base_agent\n",
    "from pysc2.lib import actions\n",
    "\n",
    "class SimpleAgent(base_agent.BaseAgent):\n",
    "    def step(self, obs):\n",
    "        super(SimpleAgent, self).step(obs)\n",
    "        \n",
    "        return actions.FunctionCall(actions.FUNCTIONS.no_op.id, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you follow the author's instruction and create a [``simple_agent.py``](https://github.com/skjb/pysc2-tutorial/blob/master/Building%20a%20Basic%20Agent/simple_agent.py) and run\n",
    "```cmd\n",
    "python -m pysc2.bin.agent \\\n",
    "--map Simple64 \\\n",
    "--agent simple_agent.SimpleAgent \\\n",
    "--agent_race T\n",
    "```\n",
    "\n",
    "You will get something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=\"320\" height=\"240\" controls>\n",
       "  <source src=\"simple_agent.mp4\" type=\"video/mp4\">\n",
       "</video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<video width=\"320\" height=\"240\" controls>\n",
    "  <source src=\"simple_agent.mp4\" type=\"video/mp4\">\n",
    "</video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Existing Reproduction\n",
    "\n",
    "Some of the popular (most starred) reproduction repos are but not limited to:\n",
    "* [pysc2-examples](https://github.com/chris-chris/pysc2-examples)\n",
    "* [pysc2-agents](https://github.com/xhujoy/pysc2-agents)\n",
    "* [sc2aibot](https://github.com/pekaalto/sc2aibot)\n",
    "* [pysc2-RLagents](https://github.com/greentfrapp/pysc2-RLagents)\n",
    "\n",
    "Unfortunately, most of the implementation are very hard to understand. Common issues includes \n",
    "\n",
    "* Unconventional project setup\n",
    "    * For example, ``pysc2-examples``, one of the most starred reproduction, does not follow the typical python project structure defined [here](https://packaging.python.org/tutorials/distributing-packages/#initial-files). As a result, it seems that one can only use Pycharm to run the project.\n",
    "* Poor documentation\n",
    "    * For example, ``sc2aibot`` only provides a documentation for the [agent parameters](https://github.com/pekaalto/sc2aibot/blob/master/actorcritic/agent.py), and all the other functions are left undocumented. Because of this, it's even hard to figure out how the observation is passed to the model.\n",
    "* Tight coupling/ Large functions\n",
    "    * For example, ``pysc2-RLagents`` use [one giant file](https://github.com/greentfrapp/pysc2-RLagents/blob/master/Agents/PySC2_A3C_AtariNet.py) to include \"everything\" (the training, runing, reacting components of the algorithm). Deeply nested loop and condition are ubiquitous and, as a result, the author's work is almost unreadable. \n",
    "    \n",
    "Nonetheless, I would like to point out that ``sc2aibot`` is probably the best out of those repos. Unlike other repos,  ``sc2aibot`` almost reproduces most of the Mini-games and shows a decent results compared to Deepmind's results:\n",
    "\n",
    "\n",
    "<center>\n",
    "<br>\n",
    "<table align=\"center\">\n",
    "  <tr>\n",
    "        <td align=\"center\">Map</td>\n",
    "        <td align=\"center\">Avg score</td>\n",
    "        <td align=\"center\">Deepmind avg</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td align=\"center\">MoveToBeacon</td>\n",
    "        <td align=\"center\">25</td>\n",
    "        <td align=\"center\">26</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td align=\"center\">CollectMineralShards</td>\n",
    "        <td align=\"center\">91</td>\n",
    "        <td align=\"center\">103</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td align=\"center\">DefeatZerglingsAndBanelings</td>\n",
    "      <td align=\"center\">48</td>\n",
    "      <td align=\"center\">62</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td align=\"center\">FindAndDefeatZerglings</td>\n",
    "      <td align=\"center\">42</td>\n",
    "      <td align=\"center\">45</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td align=\"center\">DefeatRoaches</td>\n",
    "      <td align=\"center\">70-90</td>\n",
    "      <td align=\"center\">100</td>\n",
    "    </tr>\n",
    "</table>\n",
    "<br>\n",
    "<span><i>Table. The comparision between the results from Deepmind and sc2aibot</i></span>\n",
    "</center>\n",
    "\n",
    "I ran ``sc2aibot`` implementation on ``CollectMineralShards`` mini-games for 12 hours, yet the environment only ran for 5,000 episodes and the average reward is still around 20. Since he had already ran such algorithms with 56,000 episodes, I was not interested in running it.\n",
    "\n",
    "<center>\n",
    "<img src=\"screenshots/sc2aibot_CollectMineralShards_training_time.png\" width=\"600\" height=\"600\" />\n",
    "<br>\n",
    "<span><i>Picture. The training episodes (x-axis) and scores (y-axis)</i></span>\n",
    "</center>\n",
    "\n",
    "Notice in his screenshot, the training time only took about 8 hours. This is probably due to his more powerful computer and parallel trainings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproduction through OpenAI Gym and Tensorforce\n",
    "\n",
    "From a software engineering perspective, we want to write modular code for maintainabiliy and understandibily. Because of this, we want to seperate the task of training agents into two subsystem: the system that deals with gaming environment and the system that deals with agents training.\n",
    "\n",
    "### The subsystem that deals with the gaming environment\n",
    "\n",
    "Fortunately, there exist such systems/librarys that are well structured and documented. For example, OpenAI Gym is a python RL toolkit that gives you access to a standardized set of environments <cite data-cite=\"Brockman2016-dq\"> (Brockman, 2016)</cite>. Each environment is expected to have a standardized set of methods for user to call. A typical environment would look like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# https://github.com/openai/gym/tree/master/gym/envs\n",
    "import gym\n",
    "from gym import error, spaces, utils\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "\n",
    "class FooEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        self.observation_space = spaces.Box(low, high)\n",
    "\n",
    "    def _step(self, action):\n",
    "        pass\n",
    "    \n",
    "    def _reset(self):\n",
    "        pass\n",
    "    \n",
    "    def _render(self, mode='human', close=False):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of such setup, one can find out explicitly what the valid actions and observation look like by using ``action_space`` and ``observation_space``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_space = spaces.Discrete(2)\n",
    "# The action_space only has two discrete actions: 0 and 1\n",
    "print(action_space)\n",
    "action_space.contains(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using OpenAI Gym, we enhance the robostness and predictability of the environment. I was planning on creating a gym wrapper/binding with ``pysc2``, but luckily someone has already made such library: [``sc2gym``](https://github.com/islamelnabarawy/sc2gym). It enables us \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#blablabla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The subsytem that deals with the training process\n",
    "\n",
    "One of the shortfall of the existing reproduction is that they hard-coded the training process includes predicting an action, analyzing rewards, updating action policy and etc. Instead of reinventing the wheels, we should use reinforcement learning libraries such as Tensorforce and Keras-Rl that are readily available. Their training process is much more robost because the underlying codebase is usually well-tested.\n",
    "\n",
    "After much comparison, I chose Tensorforce because of its flexibility.\n",
    "> TensorForce is an open source reinforcement learning library focused on providing clear APIs, readability and modularisation to deploy reinforcement learning solutions both in research and practice. <cite data-cite=\"Schaarschmidt2017-ur\"> (Schaarschmidt, 2017)</cite>\n",
    "\n",
    "A typical example involves Openai Gym looks like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://github.com/reinforceio/tensorforce/blob/master/examples/quickstart.py\n",
    "import numpy as np\n",
    "\n",
    "from tensorforce.agents import PPOAgent\n",
    "from tensorforce.execution import Runner\n",
    "from tensorforce.contrib.openai_gym import OpenAIGym\n",
    "\n",
    "# Create an OpenAIgym environment\n",
    "env = OpenAIGym('CartPole-v0', visualize=True)\n",
    "\n",
    "# Network as list of layers\n",
    "network_spec = [\n",
    "    dict(type='dense', size=32, activation='tanh'),\n",
    "    dict(type='dense', size=32, activation='tanh')\n",
    "]\n",
    "\n",
    "agent = PPOAgent(\n",
    "    states_spec=env.states,\n",
    "    actions_spec=env.actions,\n",
    "    network_spec=network_spec,\n",
    "    batch_size=4096,\n",
    "    # Agent\n",
    "    preprocessing=None,\n",
    "    exploration=None,\n",
    "    reward_preprocessing=None,\n",
    "    # BatchAgent\n",
    "    keep_last_timestep=True,\n",
    "    # PPOAgent\n",
    "    step_optimizer=dict(\n",
    "        type='adam',\n",
    "        learning_rate=1e-3\n",
    "    ),\n",
    "    optimization_steps=10,\n",
    "    # Model\n",
    "    scope='ppo',\n",
    "    discount=0.99,\n",
    "    # DistributionModel\n",
    "    distributions_spec=None,\n",
    "    entropy_regularization=0.01,\n",
    "    # PGModel\n",
    "    baseline_mode=None,\n",
    "    baseline=None,\n",
    "    baseline_optimizer=None,\n",
    "    gae_lambda=None,\n",
    "    normalize_rewards=False,\n",
    "    # PGLRModel\n",
    "    likelihood_ratio_clipping=0.2,\n",
    "    summary_spec=None,\n",
    "    distributed_spec=None\n",
    ")\n",
    "\n",
    "# Create the runner\n",
    "runner = Runner(agent=agent, environment=env)\n",
    "\n",
    "\n",
    "# Callback function printing episode statistics\n",
    "def episode_finished(r):\n",
    "    print(\"Finished episode {ep} after {ts} timesteps (reward: {reward})\".format(ep=r.episode, ts=r.episode_timestep,\n",
    "                                                                                 reward=r.episode_rewards[-1]))\n",
    "    return True\n",
    "\n",
    "\n",
    "# Start learning\n",
    "runner.run(episodes=10, max_episode_timesteps=200, episode_finished=episode_finished)\n",
    "\n",
    "# Print statistics\n",
    "print(\"Learning finished. Total episodes: {ep}. Average reward of last 100 episodes: {ar}.\".format(\n",
    "    ep=runner.episode,\n",
    "    ar=np.mean(runner.episode_rewards[-100:]))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the way, the reason you can use ``env = OpenAIGym('CartPole-v0', visualize=True)`` to visualzie the training process of OpenAI Gym with tensorforce is because I made a [pull request](https://github.com/reinforceio/tensorforce/pull/242). You are welcome :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Attempt\n",
    "\n",
    "With the tools ready, I gave it a try with the following code. (Don't try to run the code in jupyter notebook, because it just keeps printing stuff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tensorforce.agents import PPOAgent\n",
    "from tensorforce.execution import Runner\n",
    "from tensorforce.contrib.openai_gym import OpenAIGym\n",
    "\n",
    "import sc2gym\n",
    "from absl import flags\n",
    "FLAGS = flags.FLAGS\n",
    "FLAGS([__file__])\n",
    "\n",
    "\n",
    "# Create an OpenAIgym environment\n",
    "# ReversedAddition-v0\n",
    "# CartPole-v0\n",
    "env = OpenAIGym('SC2CollectMineralShards-v2', visualize=False)\n",
    "\n",
    "# Network as list of layers\n",
    "network_spec = [\n",
    "    dict(type='conv2d', size=32),\n",
    "    dict(type='flatten'),\n",
    "    dict(type='dense', size=32, activation='relu'),\n",
    "    dict(type='lstm', size=32)\n",
    "]\n",
    "\n",
    "saver_spec = {\n",
    "    'load': True,\n",
    "    'file': 'model.ckpt-7914479',\n",
    "    'directory': './model',\n",
    "    'seconds': 3600\n",
    "}\n",
    "\n",
    "agent = PPOAgent(\n",
    "    states_spec=env.states,\n",
    "    actions_spec=env.actions,\n",
    "    network_spec=network_spec,\n",
    "    batch_size=10,\n",
    "    # Agent\n",
    "    preprocessing=None,\n",
    "    exploration=None,\n",
    "    reward_preprocessing=None,\n",
    "    saver_spec=saver_spec,\n",
    "    # BatchAgent\n",
    "    keep_last_timestep=True,\n",
    "    # PPOAgent\n",
    "    step_optimizer=dict(\n",
    "        type='adam',\n",
    "        learning_rate=1e-4,\n",
    "        epsilon=5e-7\n",
    "    ),\n",
    "    optimization_steps=10,\n",
    "    # Model\n",
    "    scope='ppo',\n",
    "    discount=0.99,\n",
    "    # DistributionModel\n",
    "    distributions_spec=None,\n",
    "    entropy_regularization=0.01,\n",
    "    # PGModel\n",
    "    baseline_mode=None,\n",
    "    baseline=None,\n",
    "    baseline_optimizer=None,\n",
    "    gae_lambda=None,\n",
    "    normalize_rewards=False,\n",
    "    # PGLRModel\n",
    "    likelihood_ratio_clipping=0.2,\n",
    "    summary_spec=None,\n",
    "    distributed_spec=None\n",
    ")\n",
    "    \n",
    "print('partially success')\n",
    "\n",
    "# Create the runner\n",
    "runner = Runner(agent=agent, environment=env)\n",
    "\n",
    "\n",
    "# Callback function printing episode statistics\n",
    "rewards = []\n",
    "def episode_finished(r):\n",
    "    print(\"Finished episode {ep} after {ts} timesteps (reward: {reward})\".format(ep=r.episode, ts=r.episode_timestep,\n",
    "                                                                                 reward=r.episode_rewards[-1]))\n",
    "    global rewards\n",
    "    rewards += [r.episode_rewards[-1]]\n",
    "    return True\n",
    "\n",
    "\n",
    "# Start learning\n",
    "runner.run(episodes=60000, episode_finished=episode_finished)\n",
    "\n",
    "# Print statistics\n",
    "print(\"Learning finished. Total episodes: {ep}. Average reward of last 100 episodes: {ar}.\".format(\n",
    "    ep=runner.episode,\n",
    "    ar=np.mean(runner.episode_rewards[-100:]))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproduction through "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style> \n",
       "code {\n",
       "    background-color : #eff0f1 !important;\n",
       "    padding: 1px 5px !important;\n",
       "}\n",
       "\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<style> \n",
    "code {\n",
    "    background-color : #eff0f1 !important;\n",
    "    padding: 1px 5px !important;\n",
    "}\n",
    "\n",
    "</style>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
